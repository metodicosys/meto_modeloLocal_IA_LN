{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262013da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# +METO+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55667f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade openai\n",
    "#pip install PyMuPDF\n",
    "#pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funciona bien solo que no esta separando solo a textos importantes\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "\n",
    "pdf_path = Path(\"C:\\\\meto\\\\mi documento.pdf\")\n",
    "\n",
    "# Comprobar si el PDF existe\n",
    "if pdf_path.exists():\n",
    "    # Función para extraer texto del PDF\n",
    "    def extraer_texto_pdf(pdf_path,pagina_inicio=2):\n",
    "        texto = \"\"\n",
    "        with fitz.open(pdf_path) as pdf:\n",
    "            for pagina in pdf:\n",
    "                # Comenzar a extraer desde la página especificada\n",
    "                if pagina.number >= pagina_inicio:\n",
    "                    texto += pagina.get_text() + \"\\n\"\n",
    "                 #  texto += \"\\n\"  # Agregar un salto de línea entre páginas\n",
    "\n",
    "        texto = texto.replace(\"...\", \"\")  # Eliminar puntos suspensivos\n",
    "        # Limpiar texto\n",
    "        texto = ' '.join(texto.split())  # Reemplazar múltiples espacios por uno solo\n",
    "                # Filtrar texto para obtener solo contenido relevante\n",
    "        lineas_relevantes = []\n",
    "        for linea in texto.split('\\n'):\n",
    "            if re.search(r'\\b(importante|relevante|objetivo|política)\\b', linea, re.IGNORECASE):\n",
    "                lineas_relevantes.append(linea)\n",
    "\n",
    "        # Unir las líneas relevantes\n",
    "        texto_relevante = '\\n'.join(lineas_relevantes)\n",
    "        # Estructurar el texto relevante\n",
    "        secciones = re.split(r'(\\d+\\.\\s)', texto_relevante)  # Separar por secciones numeradas\n",
    "        texto_organizado = \"\"\n",
    "\n",
    "        for i in range(1, len(secciones), 2):  # Iterar por las secciones\n",
    "            encabezado = secciones[i].strip()\n",
    "            contenido = secciones[i + 1].strip() if i + 1 < len(secciones) else \"\"\n",
    "            texto_organizado += f\"{encabezado} {contenido}\\n\\n\"\n",
    "\n",
    "        return texto_organizado\n",
    "\n",
    "    # Extraer el texto\n",
    "    texto_pdf = extraer_texto_pdf(pdf_path, pagina_inicio=2)\n",
    "    print(texto_pdf)\n",
    "\n",
    "    # Imprimir longitud del texto\n",
    "    print(f\"Longitud del texto extraído: {len(texto_pdf)} caracteres\")\n",
    "    # Imprimir los primeros 100 caracteres\n",
    "    print(f\"Primeros 100 caracteres:\\n{texto_pdf[:100]}\")\n",
    "else:\n",
    "    print(\"El archivo PDF no existe en la ruta especificada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c01271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import re\n",
    "import fitz\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    # Elimina caracteres especiales, dejando solo letras, números y espacios\n",
    "    texto = re.sub(r'[...]+', '', texto)  # Eliminar puntos suspensivos\n",
    "    texto = re.sub(r'(\\d+)\\s+(\\d+)', r'\\1.\\2', texto)\n",
    "    texto = re.sub(r'[^a-zA-Z0-9áéíóúñÑ.\\s./]', '', texto)\n",
    "    return ' '.join(texto.split())\n",
    "\n",
    "texto = \"\"\n",
    "# Abre el documento PDF y extrae texto\n",
    "with fitz.open(pdf_path) as doc:\n",
    "    for pagina in doc:\n",
    "        texto += pagina.get_text() + \"\\n\"\n",
    "\n",
    "\n",
    "texto_limpio = limpiar_texto(texto)\n",
    "print(f\"Longitud del texto limpio: {len(texto_limpio)}\")\n",
    "\n",
    "documents = [Document(page_content=texto_limpio)]\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=30)\n",
    "\n",
    "split_documents = text_splitter.split_documents(documents)#ok\n",
    "#print(texto_limpio)\n",
    "print(f\"Longitud del texto original: {len(texto)}\")\n",
    "print(f\"Número de chunks: {len(split_documents)}\")\n",
    "\n",
    "# Imprime los chunks\n",
    "for i, chunk in enumerate(split_documents):\n",
    "    print(f\"Chunk {i}: {chunk.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1cb2fa",
   "metadata": {},
   "source": [
    "## Divide documento en partes (*chunks*)\n",
    "\n",
    "Esto cumple dos funciones:\n",
    "\n",
    " * Mejorar la relevancia semántica de nuestros embeddings: un documento muy grande puede cubrir demasiados temas, mientras que uno pequeño puede estar más enfocado en un solo tópico\n",
    " * Facilitar la tarea del modelo de lenuaje generativo – *chunks* más pequeños hacen que la ventana de contexto sea más pequeña\n",
    "\n",
    "El proceso de división tiene varios parámetros: el tamaño del *chunk* y el tamaño de traslape entre *chunks*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f708fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings()\n",
    "query_result = embeddings.embed_query(split_documents[0].page_content)\n",
    "query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0142cda",
   "metadata": {},
   "source": [
    "## Guardamos a la base de datos tipo vectorial Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(split_documents, embeddings)\n",
    "\n",
    "# Verifica si el vectorstore se ha creado correctamente\n",
    "print(f\"Vectorstore creado con {len(vectorstore)} vectores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaa8cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline  # Importa Hugging Face\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Inicializar el modelo Llama 3\n",
    "llm = Ollama(model=\"llama3\", temperature=0.1)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    vectorstore.as_retriever(),\n",
    "   # return_source_document = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecc7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "cache = {}\n",
    "#query = \"Por favor, responde únicamente en español: Dime los expedientes por categoria por aportaciones a Praxis\"\n",
    "query = \"Por favor, responde únicamente en español: Dime los Expedientes por Categoría, que corresponde al documento Circulo de excelencia y compromiso PLRH-0313\"\n",
    "\n",
    "# Verificar si la consulta ya está en el caché\n",
    "if query in cache:\n",
    "    respuesta = cache[query]\n",
    "    print(\"Respuesta desde caché:\", respuesta)\n",
    "else:\n",
    "    # Si no está en el caché, hacer la consulta al modelo\n",
    "    inputs = {\n",
    "        \"question\": query,\n",
    "        \"chat_history\": chat_history\n",
    "    }\n",
    "    print(inputs) \n",
    "\n",
    "    result = qa(inputs)\n",
    "    print(\"Resultado completo:\", result)  # Imprime el resultado completo para inspección\n",
    "#result = qa({\"Question\": query, \"chat_history\": chat_history})\n",
    "if \"answer\" in result:\n",
    "    respuesta = result[\"answer\"]\n",
    "    print(\"Respuesta:\", respuesta)\n",
    "    # Actualizar el historial de chat\n",
    "    chat_history.append((query, respuesta))\n",
    "else:\n",
    "    print(\"No se obtuvo respuesta.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el historial de chat\n",
    "print(\"Historial de chat:\")\n",
    "for pregunta, respuesta in chat_history:\n",
    "    print(f\"Pregunta: {pregunta}\\nRespuesta: {respuesta}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
